# -*- coding: utf-8 -*-
"""DM-2.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KPO-EymkOEMrDGoHYFFZquU4kT7pmw7M
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader, Dataset

# Load data
train_data = pd.read_csv('train.csv')
test_data = pd.read_csv('test.csv')

print(train_data.head())

# Get user and item ids
users = train_data['user-id'].unique()
items = train_data['movie-id'].unique()

# Create a mapping from user/item ids to matrix indices
user_to_index = {user: i for i, user in enumerate(users)}
item_to_index = {item: i for i, item in enumerate(items)}

n_users = len(users)
n_items = len(items)

# Create the recommendation score matrix
recommendation_matrix = np.zeros((n_users, n_items))

# Populate the recommendation matrix
for _, row in train_data.iterrows():
    user = row['user-id']
    item = row['movie-id']
    score = row['recommendation-score']
    recommendation_matrix[user_to_index[user], item_to_index[item]] = score

# Compute user means
user_mean = np.true_divide(recommendation_matrix.sum(1), (recommendation_matrix != 0).sum(1))
# Compute item means
item_mean = np.true_divide(recommendation_matrix.sum(0), (recommendation_matrix != 0).sum(0))

# Center the recommendation matrix by subtracting user and item means
for i in range(n_users):
    recommendation_matrix[i, recommendation_matrix[i, :] != 0] -= user_mean[i]

for j in range(n_items):
    recommendation_matrix[recommendation_matrix[:, j] != 0, j] -= item_mean[j]

# Predict function
def predict(user, movie):
    if user in user_to_index and movie in item_to_index:
        user_idx = user_to_index[user]
        movie_idx = item_to_index[movie]
        return recommendation_matrix[user_idx, movie_idx] + user_mean[user_idx] + item_mean[movie_idx]
    else:
        return user_mean[user_to_index[user]]  # Fallback to user mean if movie is unknown

# Make predictions for test data using the mean-centered recommendation matrix
test_data['predicted_score'] = test_data.apply(lambda row: predict(row['user-id'], row['movie-id']), axis=1)

# PyTorch model for Gradient Descent
class MF(nn.Module):
    def __init__(self, n_users, n_items, n_factors=20):
        super().__init__()
        self.user_factors = nn.Embedding(n_users, n_factors)
        self.item_factors = nn.Embedding(n_items, n_factors)

    def forward(self, user, item):
        return (self.user_factors(user) * self.item_factors(item)).sum(1)

# Create custom dataset class
class RatingsDataset(Dataset):
    def __init__(self, data, user_to_index, item_to_index):
        self.data = data
        self.user_to_index = user_to_index
        self.item_to_index = item_to_index

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        user_id = self.user_to_index[self.data.iloc[idx]['user-id']]
        item_id = self.item_to_index[self.data.iloc[idx]['movie-id']]
        rating = self.data.iloc[idx]['recommendation-score']
        return torch.LongTensor([user_id]), torch.LongTensor([item_id]), torch.FloatTensor([rating])

# Prepare dataset and DataLoader for Gradient Descent
train_dataset = RatingsDataset(train_data, user_to_index, item_to_index)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)

# Gradient Descent Training
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = MF(n_users, n_items).to(device)
optimizer = optim.Adam(model.parameters())
criterion = nn.MSELoss()

n_epochs = 10
gd_losses = []

for epoch in range(n_epochs):
    model.train()
    total_loss = 0
    for user, item, rating in train_loader:
        user = user.to(device)
        item = item.to(device)
        rating = rating.to(device)

        optimizer.zero_grad()
        prediction = model(user, item)
        loss = criterion(prediction.view(-1), rating)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    avg_loss = total_loss / len(train_loader)
    gd_losses.append(avg_loss)
    print(f'Epoch {epoch + 1}/{n_epochs} GD Loss: {avg_loss:.4f}')

# Plot loss convergence for Gradient Descent
plt.figure()
plt.plot(gd_losses, label='Gradient Descent')

# Make predictions for test data using Gradient Descent model
model.eval()
gd_test_predictions = []
for _, row in test_data.iterrows():
    user = torch.LongTensor([user_to_index[row['user-id']]]).to(device)
    item = torch.LongTensor([item_to_index[row['movie-id']]]).to(device)
    prediction = model(user, item).item()
    gd_test_predictions.append(prediction)

test_data['predicted_score_gd'] = gd_test_predictions

# Save predictions to a text file
test_data[['user-id', 'movie-id', 'predicted_score_gd']].to_csv('predictions_gd.txt', index=False, header=False)

# Implementing ALS with Loss Tracking
def als(train_data, user_to_index, item_to_index, n_factors=20, n_epochs=10):
    n_users = len(user_to_index)
    n_items = len(item_to_index)

    # Initialize user and item factors
    user_factors = np.random.normal(scale=1./n_factors, size=(n_users, n_factors))
    item_factors = np.random.normal(scale=1./n_factors, size=(n_items, n_factors))

    als_losses = []  # To store the ALS loss for each epoch

    # Perform ALS
    for epoch in range(n_epochs):
        # Update user factors
        for i in range(n_users):
            item_idx = recommendation_matrix[i, :] > 0
            if np.sum(item_idx) > 0:
                user_factors[i] = np.linalg.solve(np.dot(item_factors[item_idx].T, item_factors[item_idx]) + 0.1 * np.eye(n_factors),
                                                   np.dot(item_factors[item_idx].T, recommendation_matrix[i, item_idx]))

        # Update item factors
        for j in range(n_items):
            user_idx = recommendation_matrix[:, j] > 0
            if np.sum(user_idx) > 0:
                item_factors[j] = np.linalg.solve(np.dot(user_factors[user_idx].T, user_factors[user_idx]) + 0.1 * np.eye(n_factors),
                                                   np.dot(user_factors[user_idx].T, recommendation_matrix[user_idx, j]))

        # Calculate and store the loss (mean squared error)
        predicted_matrix = np.dot(user_factors, item_factors.T)
        mask = recommendation_matrix != 0
        mse_loss = np.mean((recommendation_matrix[mask] - predicted_matrix[mask]) ** 2)
        als_losses.append(mse_loss)
        print(f'Epoch {epoch + 1}/{n_epochs} ALS Loss: {mse_loss:.4f}')

    return user_factors, item_factors, als_losses

# Training using ALS
als_user_factors, als_item_factors, als_losses = als(train_data, user_to_index, item_to_index)

# Make predictions for test data using ALS
als_test_predictions = []
for _, row in test_data.iterrows():
    user_idx = user_to_index[row['user-id']]
    item_idx = item_to_index[row['movie-id']]
    prediction = np.dot(als_user_factors[user_idx], als_item_factors[item_idx]) + user_mean[user_idx] + item_mean[item_idx]
    als_test_predictions.append(prediction)

test_data['predicted_score_als'] = als_test_predictions

# Save ALS predictions to a text file
test_data[['user-id', 'movie-id', 'predicted_score_als']].to_csv('predictions_als.txt', index=False, header=False)

# Plot loss convergence for both Gradient Descent and ALS
plt.plot(als_losses, label='ALS')
plt.title('Loss Convergence (Gradient Descent vs ALS)')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.savefig('loss_convergence.pdf')  # Save plot as PDF
plt.close()

print(test_data.head())